---
layout: default 
title: Tao He's Homepage
---

		<div class="blurb">
			<h3>Dr. He Tao(何涛)</h3>
			<p><img src="1.jpg" alt="photo" style="width:190px;height:226px;float:left;margin-right:35px;">
				<ul class="contacts">
					<li>Laboratory of Intelligent Collaborative Computing</li>
					<li>University of Electronic Science and Technology of China (UESTC)</li>
					 
					<li> <b>Email</b>: tao.he01@hotmail.com </li>
					<li><a href="https://scholar.google.com/citations?user=nhbZOLcAAAAJ&hl=en"><div style="color:blue;"> Google Scholar</div></a></li>
					 
				</ul>
			</p>
			</div> 

<div class="about">
	<p>Dr. Tao He is currently a Deputy Researcher (equivalent to Associate Professor) at the <a = href="https://icct.uestc.edu.cn/index.htm"><font color="#0000ff">Laboratory of Intelligent Collaborative Computing</font></a>, University of Electronic Science and Technology of China (UESTC). Prior to joining UESTC, he was a Research Fellow at Monash University, Australia.

He received his Ph.D. in Computer Science from Monash University in 2022, under the supervision of Associate Professor <a = href="https://liyuanfang.github.io"><font color="#0000ff">Yuan-Fang Li</font></a>, with co-supervision by Professor <a href="https://lianligao.github.io/"><font color="#0000ff">Lianli Gao</font></a>. He obtained his M.S. degree in Computer Science from UESTC in 2018 under the supervision of Professor <a href="https://scholar.google.com.au/citations?user=F5Zy9V4AAAAJ&hl=en"><font color="#0000ff">Jingkuan Song</font></a>, and his B.S. degree in Computer Science from Northeast Normal University (NENU) in 2015.

Dr. Tao's research interests lie in the field of computer vision, with a particular focus on scene graph generation, human-object interaction detection, and image retrieval. </p> 
<font color="#0000ff"> Accepting highly self-motivated Ph.D students. </font>
<!-- 	<p>I am a Deputy Reseacher(equivalent to Associate Professor) at <a = href="https://icct.uestc.edu.cn/index.htm"><font color="#0000ff">Laboratory of Intelligent Collaborative Computing</font></a>, UESTC. Before working at UESTC, I was a research fellow at Monash University. I earned my Ph.D. at Monash University in 2022 under the supervision of 
		<a = href="https://users.monash.edu.au/~yli/"><font color="#0000ff">Assoc. Prof. Yuan-Fang Li</font></a>  
		and co-supervision with <a href="https://lianligao.github.io/"><font color="#0000ff">Prof. Lianli Gao</font>.</a> 
	I obtained my MS and BS degrees in Computer Science from UESTC  under the supervision of <a href="https://scholar.google.com.au/citations?user=F5Zy9V4AAAAJ&hl=en"><font color="#0000ff">Prof. Jingkuan Song</font></a> in 2018,  and Northeast Normal University (NENU) in 2015, respectively. My research interests focus on computer vision, such as scene graph generation, human-object interaction detection, and image retrieval etc.   -->
<!-- 	</p>  -->
	
<h4> Recent News</h4>
<ul>
	<li>Two papers accepted to <strong>ICCV 2025</strong> on Panoptic Scene Graph Generation and Incomplete Multimodal Learning.</li>
	<li>One paper accepted to <strong>CVPR 2025</strong> on Visual Emotion Recognition.</li>
	<li>Two papers accepted to <strong>ICASSP 2025</strong> on Audio-to-Intent Recognition.</li>
	<li>Two papers accepted to <strong>ACM MM 2024</strong> on scene graph generation and model compression.</li>
	<li>One paper accepted to <strong>TIP 2023</strong> on scene graph generation.</li>
	<li>One paper accepted to <strong>TIP 2022</strong> on unbiased scene graph generation.</li>
	<li>One paper accepted to <strong>ECCV 2022</strong>  on open-vocabulary scene graph generation.</li>
	<li>Starting my postdoc in the <a = href="https://www.monash.edu/it/dsai/vision-language"><strong> <font color="#0000ff">Vision and Language group </font></strong></a> from Feb. 2022.</li>
	<li>One paper accepted to <strong>TNNLS 2021</strong>  on network embedding.</li>
	<li>One paper accepted to <strong>ICCV 2021</strong>  on Human-Ojbect interaction with scene graphs.</li>
<!-- 	<li>One paper acccepted to <strong>IJCAI 2021</strong>  on scene graph generation.</li> -->
<!-- 	<li>One paper accepted to <strong>AAAI 2020</strong>  on network quantisation.</li> -->
<!-- 	<li>One paper acccepted to <strong>IJCV 2020</strong>  on image compression.</li> -->
	<!--<li>One paper accepted to <strong>IJCAI 2019</strong>  on domain adaptive hash for images.</li>--!>
	</ul>

<h4>Selected Publications</h4>
<ul>
	<li>Xin Hu, Ke Qin, Guiduo Duan, Ming Li, Yuan-Fang Li, <strong>Tao He*</strong>. SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning. <strong>ICCV 2025.</strong></li>
	<li>Ruiting Dai, Chenxi Li, Yandong Yan, Lisi Mo, Ke Qin, <strong>Tao He*</strong>. Unbiased Missing-modality Multimodal Learning.  <strong>ICCV 2025</strong>.</li>
	<li>Wen Yin, Yong Wang, Guiduo Duan, Dongyang Zhang, Xin Hu, Yuan-Fang Li, <strong>Tao He*</strong>. Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition.  <strong>CVPR 2025</strong>. <a href="https://yinwen2019.github.io/ucdver/" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://www.arxiv.org/abs/2505.19694" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li.  Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection. TIP 2023. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="~" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li.  State-aware Compositional Learning towards Unbiased Training for Scene Graph Generation. TIP 2022. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="~" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li. Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning. ECCV 2022. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="~" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li. Exploiting Scene Graphs for Human-Object Interaction Detection. ICCV 2021.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/abs/2108.08584" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li.Semisupervised Network Embedding With Differentiable Deep Quantization. TNNLS 2021. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/pdf/2108.09128" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li>Jingkuan Song, <strong>Tao He</strong>, Lianli Gao,  Alan Hanjalic, Hengtao Shen. Unified Binary Generative Adversarial Network for Image Retrieval and Compression. IJCV 2020.<a href="" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="http://pure.tudelft.nl/ws/files/82393102/ijcv2020_binary.pdf" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Xing Wang, Ke Huang, Yuan-Fang Li. Sneq: Semi-supervised Attributed Network Embedding with Attention-based Quantisation. AAAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li. Learning from the scene and borrowing from the rich: Tackling the long tail in scene graph generation. IJCAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>


<!-- 	<li><strong>Tao He</strong>, Yuan-Fang Li, Lianli Gao, Dongxiang Zhang, Jingkuan Song. Learning from the scene and borrowing from the rich: Tackling the long tail in scene graph generation. IJCAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
 -->
<!-- 	<li>Jingkuan Song, <strong>Tao He</strong>, Lainli Gao, Xing Xu, Hengtao Shen. Deep region hashing for efficient large-scale instance search from images. AAAI 2018.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li> -->
<!-- 	<li>Jingkuan Song, <strong>Tao He</strong>, Lainli Gao, Xing Xu, Alan Hanjalic, Hengtao Shen. Binary generative adversarial networks for image retrieval. AAAI 2018.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li> -->
<!-- 	<li>Jingkuan Song, <strong>Tao He</strong>, Hangbo Fan, Lianli Gao. Deep discrete hashing with self-supervised pairwise labels. ECML 2017.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li> -->
	
</ul>

	
<h4> Acadamic Service </h4>
<ul>
	<li>As a reviewer for conferences: CVPR, ECCV, ICCV, CVPR, AAAI, ACMMM, CIKM</li>
	<li>As a reviewer for journals: TIP, TNNLS, TPAMI, IJCV etc. </li>
	</ul>
</div>
