---
layout: default 
title: Tao He's Homepage
---

		<div class="blurb">
			<h3>Dr. He Tao(何涛)</h3>
			<p><img src="1.jpg" alt="photo" style="width:190px;height:226px;float:left;margin-right:35px;">
				<ul class="contacts">
					<li>Faculty of Information Technology</li>
					<li>Monash University</li>
					<li> VIC, 3800, Australia </li>
					<li> <b>Email</b>: tao.he01@hotmail.com </li>
					<li><a href="https://scholar.google.com/citations?user=nhbZOLcAAAAJ&hl=en"><div style="color:blue;"> Google Scholar</div></a></li>
					 
				</ul>
			</p>
			</div> 

<div class="about">
	
	<p>I earned my Ph.D. at Monash University in 2022 under the supervision of 
		<a = href="https://users.monash.edu.au/~yli/"><font color="#0000ff">Assoc. Prof. Yuan-Fang Li</font></a>  
		and co-supervision with <a href="https://lianligao.github.io/"><font color="#0000ff">Prof. Lianli Gao</font>.</a> I am now a research fellow  
		of the <a = href="https://www.monash.edu/it/dsai/vision-language"><strong> <font color="#0000ff">Vision and Language group</font></strong></a>. &nbsp;
	I obtained my MS and BS degrees in Computer Science from  University of Electronic Science and Technology of China (UESTC)  under the supervision of <a href="https://scholar.google.com.au/citations?user=F5Zy9V4AAAAJ&hl=en"><font color="#0000ff">Prof. Jingkuan Song</font></a> in 2018,  and Northeast Normal University (NENU) in 2015, respectively.  </p>
	<p> My research ranges from visual relationship detection, scene graph generation, discrete network embedding, and image retrieval, etc. 

<h4> Recent News</h4>
<ul>
	<li>One paper accepted to <strong>TIP    </strong> 2022 on unbiased scene graph generation</li>
	<li>One paper accepted to <strong>ECCV  </strong> 2022 on open-vocabulary scene graph generation</li>
	<li>Starting my postdoc in the <a = href="https://www.monash.edu/it/dsai/vision-language"><strong> <font color="#0000ff">Vision and Language group </font></strong></a> from Feb. 2022.</li>
	<li>One paper accepted to <strong>TNNLS</strong> 2021 on network embedding</li>
	<li>One paper accepted to <strong>ICCV</strong> 2021 on Human-Ojbect interaction with scene graphs.</li>
	<li>One paper acccepted to <strong>IJCAI</strong> 2021 on scene graph generation.</li>
	<li>One paper accepted to <strong>AAAI</strong> 2020 on network quantisation.</li>
	<li>One paper acccepted to <strong>IJCV</strong> 2020 on image compression.</li>
	<!--<li>One paper accepted to <strong>IJCAI</strong> 2019 on domain adaptive hash for images.</li>--!>
	</ul>

<h4>Publications</h4>
<ul>
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li.  State-aware Compositional Learning towards Unbiased Training for Scene Graph Generation. TIP 2022. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="~" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li. Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning. ECCV 2022. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="~" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li. Exploiting Scene Graphs for Human-Object Interaction Detection. ICCV 2021.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/abs/2108.08584" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Yuan-Fang Li.Semisupervised Network Embedding With Differentiable Deep Quantization. TNNLS 2021. <a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/pdf/2108.09128" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li>Jingkuan Song, <strong>Tao He</strong>, Lianli Gao,  Alan Hanjalic, Hengtao Shen. Unified Binary Generative Adversarial Network for Image Retrieval and Compression. IJCV 2020.<a href="" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="http://pure.tudelft.nl/ws/files/82393102/ijcv2020_binary.pdf" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Xing Wang, Ke Huang, Yuan-Fang Li. Sneq: Semi-supervised Attributed Network Embedding with Attention-based Quantisation. AAAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	
	<li><strong>Tao He</strong>, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li. Learning from the scene and borrowing from the rich: Tackling the long tail in scene graph generation. IJCAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>


	<li><strong>Tao He</strong>, Yuan-Fang Li, Lianli Gao, Dongxiang Zhang, Jingkuan Song. Learning from the scene and borrowing from the rich: Tackling the long tail in scene graph generation. IJCAI 2020.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li>Jingkuan Song, <strong>Tao He</strong>, Lainli Gao, Xing Xu, Hengtao Shen. Deep region hashing for efficient large-scale instance search from images. AAAI 2018.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li>Jingkuan Song, <strong>Tao He</strong>, Lainli Gao, Xing Xu, Alan Hanjalic, Hengtao Shen. Binary generative adversarial networks for image retrieval. AAAI 2018.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li>Jingkuan Song, <strong>Tao He</strong>, Hangbo Fan, Lianli Gao. Deep discrete hashing with self-supervised pairwise labels. ECML 2017.<a href=" " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href=" " rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	
	
</ul>

	
<h4> Acadamic Service </h4>
<ul>
	<li>As a reviewer for confereces: CVPR, ECCV, ICCV, CVPR, AAAI, ACMMM, CIKM</li>
	<li>As a reviewer for journals: FGCS, JVIS, IJIS, JVCIR </li>
	</ul>
</div>
